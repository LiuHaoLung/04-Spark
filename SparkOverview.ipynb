{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "___\n",
    "#### What is Spark ?\n",
    "\n",
    "Spark can think of Spark as a flexible alternative to MapReduce, and it can run top of existing Hadoop distributed file system infrastructure to provide enhanced and additional functionality.\n",
    "\n",
    "Spark can use data stored in a variety of formats, such like: Cassandra, AWS S3, HDFS ... etc.\n",
    "___\n",
    "#### Different between Spark and MapReduce ?\n",
    "\n",
    "We should really be looking at Spark as an alternative to MapReduce rather than a replacement for Hadoop.\n",
    "\n",
    "It isn't intended to replace Hadoop but to provide a comprehensive and unified solution to manage different big data use cases and requirements MapReduce in difference to Spark requires files to be stored specifically into Hadoop distribution file system, Spark while it can run on top of HDFS doesn't actually require those files to be stored in that manner.\n",
    "\n",
    "Spark also can perform operations up to 100 times faster than MapReduce.\n",
    "\n",
    "MapReduce writes most data to disk after each map and reduce operation, Spark keeps most of the data in memory after each transformation, Spark can spill over to disk to disk if the memory is filled.\n",
    "\n",
    "Basically the difference here as an abstract point of view is that MapReduce has to write things to disk or hard drive, and Spark on the other hand can keep most of that data in the memory or RAM, so you can imagine that keeping it in RAM versus having to continually write to disk is much faster.\n",
    "___\n",
    "#### The core of Spark is ?\n",
    "\n",
    "At the core of Spark is the idea of a Resilient Distributed Dataset(RDD).\n",
    "\n",
    "The RDD has 4 main features:\n",
    "\n",
    "1. Distributed Collection of Data.\n",
    "2. Fault-tolerant.\n",
    "3. Parellel operation-partioned.\n",
    "4. Ability to use many data sources.\n",
    "\n",
    "The step of RDD:\n",
    "\n",
    "1. RDD objects, it can build the operator DAG.\n",
    "2. DAG scheduler, it can split graph into stages of tasks and submit each stage as ready.\n",
    "3. Task scheduler, it can launch tasks via cluster manager and retry failed or stragging tasks.\n",
    "4. Worker node, it can execute tasks and store and serve blocks.\n",
    "\n",
    "So the Spark allows to develop complex multistep data pipelines using this directed a cyclic graph pattern, and it also supports in-memory data sharing across these DAG so that different jobs can work the same data.\n",
    "\n",
    "The RDDs are immutable, lazily evaluated and cachebale, there are two types of RDD operations, these two operations are the core of what we're goint to be doing as we code with Python and Spark will be coding up transformations and actions on some sort of distributed dataset.\n",
    "\n",
    "1. Transformations.\n",
    "2. Actions.\n",
    "\n",
    "The basic actions are:\n",
    "\n",
    "1. First\n",
    "\n",
    "    * Means return the first element in the RDD.\n",
    "    \n",
    "    \n",
    "2. Collect\n",
    "\n",
    "    * Means to return all the elements of the RDD as an array at the driven program.\n",
    "    \n",
    "    \n",
    "3. Count\n",
    "    \n",
    "    * Means return the number of elements in the RDD.\n",
    "    \n",
    "    \n",
    "4. Take\n",
    "\n",
    "    * Means return an array with the first n elements of the RDD.\n",
    "    \n",
    "    \n",
    "The basic transformations are:\n",
    "\n",
    "1. Filter\n",
    "\n",
    "    * <code>RDD.filter()</code> means applies a function to each element and returns elements that evaluate to true.\n",
    "\n",
    "\n",
    "2. Map\n",
    "\n",
    "    * <code>RDD.map()</code> means transforms each element and preserves the same number of elements.\n",
    "\n",
    "    \n",
    "    \n",
    "3. FlatMap\n",
    "\n",
    "    * <code>RDD.flatmap()</code> means transforms each elements into 0-N elements and change the number of elements.\n",
    "\n",
    "    \n",
    "___\n",
    "#### What is Pair RDDs ?\n",
    "\n",
    "Often RDDs will be holding their value in tuples(key,valus), and this oftens better partitiioning of data and leads to functionality based od reduction.\n",
    "\n",
    "So there have two other methods:\n",
    "\n",
    "1. <code>Reduce()</code> means an action that will aggregate RDD elements using a function that results a single element.\n",
    "2. <code>ReduceByKey()</code> means an action that will aggregate Pair RDD elements using a function that return a Pair RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
