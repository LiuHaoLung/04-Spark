{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Overview\n",
    "___\n",
    "\n",
    "#### What is Local versus Distributed ?\n",
    "\n",
    "Own a computer or a single machine where you are restricted to the RAM or hard drive of that single machine.\n",
    "\n",
    "On the distrubuted system can have one machine controlling a distribution of multiple machines.\n",
    "\n",
    "For instance, if you have one local machine you may use all the cores on that machine, however if you leverage the power of a distributed system, you can have one smaller master node controlling a distributed system of computers that allows you to take advantage of this distributed system and actually retain more processing power and more memory than you would be able to on a single local machine.\n",
    "\n",
    "#### Overview of the distributed systemï¼š\n",
    "\n",
    "* A local process will use the computation resources of a single machine.\n",
    "* A distributed process has access to the computational resources across a number of machines connected through a network.\n",
    "* After certain point, it is easier to scale out to many lower CPU machines, than to try to scale up to a single machine with high a CPU.\n",
    "* Distributed machines also have the advantage of easily scaling, you can just add more machines.\n",
    "* They also include fault tolerance, if one machines fails, the whole network can still go on, versus a local machine if that goes down your entire work goes down.\n",
    "___\n",
    "#### What is Hadoop ?\n",
    "\n",
    "1. Hadoop is a way to distribute very large files across multiple machines.\n",
    "2. It uses the Haoppd Distributed File System(HDFS).\n",
    "3. HDFS allows a user to work with large data set.\n",
    "4. HDFS also duplicates blocks of data for fault tolerance.\n",
    "5. It also then uses MapReduce.\n",
    "6. MapReduce allows computations on that data.\n",
    "___\n",
    "#### What is HDFS ?\n",
    "\n",
    "You may have your main name node with some CPU and RAM attached to it and then you would have several other distributed machines as data nodes.\n",
    "\n",
    "HDFS will use blocks of data, with a size of 128 MB by default, each of these blocks is replicated 3 times, and the block are distributed in a way of support fault tolerance.\n",
    "\n",
    "The support fault tolerance meaning if one of these lower data nodes gets knocked out for some reason, you have your data replicted on the other nodes, so in this condition you are not going to lose any information.\n",
    "\n",
    "Smaller blocks provide more parallelization during processing, and the multiple copies of a block prevent loss of data due to a failure of a node.\n",
    "___\n",
    "#### What is MapReduce ?\n",
    "\n",
    "MapReduce is a way of splitting a computation task to a distributed set of files(such as HDFS), it consists of a Job Tracker and multiple Task Trackers.\n",
    "\n",
    "The Job Tracker sends code to run on the Task Trackers, and the Task Trackers allocate CPU for the tasks and monitor the tasks on the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
